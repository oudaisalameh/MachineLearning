# -*- coding: utf-8 -*-
"""AutoEncoder.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jv9TnofuPbId9mlEcwMd_Sys6OFlWyAu
"""

!pip install pytorch_lightning

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import pytorch_lightning as pl
import torchvision
from torch.utils.data import DataLoader

class AE(pl.LightningModule):
    def __init__(self, **kwargs):
        super().__init__()
        self.encoder_hidden_layer = nn.Linear(
            in_features=kwargs["input_shape"], out_features=128
        )
        self.encoder_output_layer = nn.Linear(
            in_features=128, out_features=128
        )
        self.decoder_hidden_layer = nn.Linear(
            in_features=128, out_features=128
        )
        self.decoder_output_layer = nn.Linear(
            in_features=128, out_features=kwargs["input_shape"]
        )

    def forward(self, features):
        activation = self.encoder_hidden_layer(features)
        activation = torch.relu(activation)
        code = self.encoder_output_layer(activation)
        code = torch.relu(code)
        activation = self.decoder_hidden_layer(code)
        activation = torch.relu(activation)
        activation = self.decoder_output_layer(activation)
        reconstructed = torch.relu(activation)
        return reconstructed

# Initialize epochs
epochs = 10

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model to the specified device
model = AE(input_shape=784).to(device)

# Create an optimizer object
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# Mean-squared error loss
criterion = torch.nn.MSELoss()

transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

# Load MNIST datasets
train_dataset = torchvision.datasets.MNIST(
    root="~/torch_datasets", train=True, transform=transform, download=True
)

test_dataset = torchvision.datasets.MNIST(
    root="~/torch_datasets", train=False, transform=transform, download=True
)

# Create data loaders
train_loader = DataLoader(
    train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True
)

test_loader = DataLoader(
    test_dataset, batch_size=32, shuffle=False, num_workers=4
)

for epoch in range(epochs):
    loss = 0
    for batch_features, _ in train_loader:
        # Reshape mini-batch data to [N, 784] matrix and load it to the active device
        batch_features = batch_features.view(-1, 784).to(device)

        # Reset gradients back to zero
        optimizer.zero_grad()

        # Compute reconstructions
        outputs = model(batch_features)

        # Compute training reconstruction loss
        train_loss = criterion(outputs, batch_features)

        # Compute accumulated gradients
        train_loss.backward()

        # Perform parameter update based on current gradients
        optimizer.step()

        # Add the mini-batch training loss to epoch loss
        loss += train_loss.item()

    # Compute the epoch training loss
    loss = loss / len(train_loader)

    # Display the epoch training loss
    print("epoch : {}/{}, loss = {:.6f}".format(epoch + 1, epochs, loss))
    import matplotlib.pyplot as plt

    # Visualize original and reconstructed images
    with torch.no_grad():
        for batch_features, _ in test_loader:
            batch_features = batch_features.view(-1, 784).to(device)
            reconstructed = model(batch_features)
            # Display the original and reconstructed images for the first batch
            num_images = 8
            plt.figure(figsize=(9, 2))
            for i in range(num_images):
                # Original image
                plt.subplot(2, num_images, i + 1)
                plt.imshow(batch_features[i].cpu().numpy().reshape(28, 28), cmap='gray')
                plt.axis('off')
                # Reconstructed image
                plt.subplot(2, num_images, i + num_images + 1)
                plt.imshow(reconstructed[i].cpu().numpy().reshape(28, 28), cmap='gray')
                plt.axis('off')
            plt.show()
            break  # Display only the first batch
    # break